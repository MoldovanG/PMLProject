{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "class FeatureProcessor:\n",
    "    def __init__(self, tweets):\n",
    "        self.tweets = tweets\n",
    "        self.tokenizer = Tokenizer(num_words=max_words)\n",
    "        # feed our tweets to the Tokenizer\n",
    "        self.tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "        # Tokenizers come with a convenient list of words and IDs\n",
    "        self.dictionary = self.tokenizer.word_index\n",
    "    \n",
    "    def convert_text_to_index_array(self,tweet):\n",
    "        # one really important thing that `text_to_word_sequence` does\n",
    "        # is make all texts the same length -- in this case, the length\n",
    "        # of the longest text in the set.\n",
    "        return [self.dictionary[word] for word in kpt.text_to_word_sequence(tweet)]\n",
    "    def get_tokenized_tweets(self):\n",
    "        allWordIndices = []\n",
    "        # for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "        for tweet in self.tweets:\n",
    "            wordIndices = self.convert_text_to_index_array(tweet)\n",
    "            allWordIndices.append(wordIndices)\n",
    "\n",
    "        # now we have a list of all tweets converted to index arrays.\n",
    "        # cast as an array for future usage.\n",
    "        allWordIndices = np.asarray(allWordIndices)\n",
    "        \n",
    "        # create one-hot matrices out of the indexed tweets\n",
    "        train_x = self.tokenizer.sequences_to_matrix(allWordIndices, mode='count')\n",
    "\n",
    "        return train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "import preprocessor as p\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, csv_file_path, mode = 'train'):\n",
    "        data_frame = pd.read_csv(csv_file_path)\n",
    "        self.data = np.array(data_frame)\n",
    "        if mode == 'train' :\n",
    "            self.ids = self.data[:,0]\n",
    "            self.labels = np.asarray(self.data[:,1:3]).astype('float32')\n",
    "            self.features = self.clean_tweets(self.data[:,3])\n",
    "        elif mode == 'test':\n",
    "            self.ids = self.data[:,0]\n",
    "            self.features = self.clean_tweets(self.data[:,1])\n",
    "        print(self.features[0])\n",
    "        feature_processor = FeatureProcessor(self.features)\n",
    "        self.preprocessed_features = feature_processor.get_tokenized_tweets() \n",
    "        self.preprocessed_features = np.asarray(self.preprocessed_features).astype('float32')\n",
    "        \n",
    "    def clean_tweets(self,tweets):\n",
    "        stemmer = SnowballStemmer(\"german\")\n",
    "        german_stop_words = stopwords.words('german')\n",
    "        new_tweets =[]\n",
    "        for tweet in tweets:\n",
    "            tweet = tweet.lower()\n",
    "            tweet = re.sub('[!?,.:\";]', '', tweet)\n",
    "            tweet = p.tokenize(tweet)\n",
    "            cleaned_tweet =[];\n",
    "            separator = \" \"\n",
    "            for word in kpt.text_to_word_sequence(tweet):\n",
    "                if word not in german_stop_words:\n",
    "                    cleaned_tweet.append(stemmer.stem(word))\n",
    "            new_tweets.append(separator.join(cleaned_tweet))\n",
    "        \n",
    "        return np.array(new_tweets)\n",
    "    \n",
    "    def get_data_set(self):\n",
    "        return self.data\n",
    "    def get_ids(self):\n",
    "        return self.ids\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "    def get_preprocessed_features(self):\n",
    "        return self.preprocessed_features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seit d vas kaputt gang bringt numb jahr unglck antwortet de spiegel isch gar nt kaputt gang bringt numb jahr pech dadruf fangt s kondom a lach s'atomkraftwerk au emoji uuuh nd schlecht emoji seisch aner frau isch fett hesch dis leb lang unglck au i fall nm lang lebsch\n",
      "mer aner party bi kolleg en neu bro findt ht sogar dr vo dne emoji\n",
      "emoji min vibi funktionkert nd emoji hesch d'batteri dri gsteckt emoji ja isch nd sgliich\n",
      "(22583,)\n",
      "(22583, 2)\n",
      "(22583,)\n",
      "(3044,)\n",
      "(3044, 2)\n",
      "(3044,)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DatasetLoader('training.csv')\n",
    "validation_loader = DatasetLoader('validation.csv')\n",
    "test_loadet = DatasetLoader('test.csv', mode='test')\n",
    "print(train_loader.get_features().shape)\n",
    "print(train_loader.get_labels().shape)\n",
    "print(train_loader.get_ids().shape)\n",
    "print(validation_loader.get_features().shape)\n",
    "print(validation_loader.get_labels().shape)\n",
    "print(validation_loader.get_ids().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import csv \n",
    "class DataSetEvaluator:\n",
    "    def calculateMSE(self, model, dataSetLoader):\n",
    "        y_pred = model.predict(dataSetLoader.get_preprocessed_features())\n",
    "        print(y_pred[0])\n",
    "        return mean_squared_error(dataSetLoader.get_labels(), y_pred)\n",
    "    def generateEvaluationFile(self, model, dataSetLoader):\n",
    "        y_pred = model.predict(dataSetLoader.get_preprocessed_features())\n",
    "        print(y_pred[0])\n",
    "        ids = dataSetLoader.get_ids()\n",
    "        file=open('submision1.txt',\"w\")\n",
    "        writes=csv.writer(file,delimiter=',',quoting=csv.QUOTE_ALL)\n",
    "        count =0\n",
    "        for row in y_pred:\n",
    "            writes.writerow([ids[int(count)],row[0],row[1]])\n",
    "            count = count + 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\george.moldovan\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass kernel=rbf as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "C:\\Users\\george.moldovan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "alpha = 10 ** -4\n",
    "kernel = \"rbf\"\n",
    "KRmodel = KernelRidge(alpha, kernel)\n",
    "KRmodel.fit(train_loader.get_preprocessed_features(),train_loader.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "LinearRegressionModel = LinearRegression()\n",
    "LinearRegressionModel.fit(train_loader.get_preprocessed_features(),train_loader.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressorChain(base_estimator=BayesianRidge())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "BayesModel = BayesianRidge()\n",
    "BayesModelWrapper = RegressorChain(BayesModel)\n",
    "BayesModelWrapper.fit(train_loader.get_preprocessed_features(),train_loader.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.603798  9.399656]\n"
     ]
    }
   ],
   "source": [
    "evaluator = DataSetEvaluator()\n",
    "evaluator.generateEvaluationFile(LinearRegressionModel, test_loadet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\george.moldovan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\george.moldovan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RegressorChain(base_estimator=LinearSVR())"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "LinearSvrModel = LinearSVR()\n",
    "# define the chained multioutput wrapper model\n",
    "LinearSvrWrapper = RegressorChain(LinearSvrModel)\n",
    "# fit the model on the whole dataset\n",
    "LinearSvrWrapper.fit(train_loader.get_preprocessed_features(),train_loader.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               256512    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 388,354\n",
      "Trainable params: 388,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "modelNN = Sequential()\n",
    "modelNN.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "modelNN.add(Dropout(0.5))\n",
    "modelNN.add(Dense(256, activation='sigmoid'))\n",
    "modelNN.add(Dropout(0.5))\n",
    "modelNN.add(Dense(2))\n",
    "modelNN.summary()\n",
    "modelNN.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 181.7995 - val_loss: 1.4079\n",
      "Epoch 2/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 10.4872 - val_loss: 1.2815\n",
      "Epoch 3/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 10.3360 - val_loss: 1.0572\n",
      "Epoch 4/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 10.2038 - val_loss: 0.9800\n",
      "Epoch 5/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 9.8844 - val_loss: 0.9824\n",
      "Epoch 6/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 9.6405 - val_loss: 0.9108\n",
      "Epoch 7/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 9.4147 - val_loss: 0.9896\n",
      "Epoch 8/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 9.3286 - val_loss: 0.8761\n",
      "Epoch 9/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 9.0128 - val_loss: 0.9594\n",
      "Epoch 10/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 8.9319 - val_loss: 0.8607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x282858206d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelNN.fit(train_loader.get_preprocessed_features(), train_loader.get_labels(),\n",
    "  batch_size=32,\n",
    "  epochs=10,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.385906  9.545205]\n"
     ]
    }
   ],
   "source": [
    "evaluator = DataSetEvaluator()\n",
    "evaluator.generateEvaluationFile(modelNN, test_loadet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.931564  9.496623]\n",
      "1.1855316\n",
      "[52.02251294  9.85830693]\n",
      "1.2740941188578512\n",
      "[51.905483  9.606482]\n",
      "1.180663\n",
      "[51.89754052  9.49469686]\n",
      "1.1586700466671107\n"
     ]
    }
   ],
   "source": [
    "evaluator = DataSetEvaluator()\n",
    "print(evaluator.calculateMSE(LinearRegressionModel, validation_loader))\n",
    "print(evaluator.calculateMSE(LinearSvrWrapper, validation_loader))\n",
    "print(evaluator.calculateMSE(modelNN, validation_loader))\n",
    "print(evaluator.calculateMSE(BayesModelWrapper, validation_loader))\n",
    "# print(evaluator.calculateMSE(KRmodel, validation_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "636/636 [==============================] - 39s 61ms/step - loss: 674.9890 - val_loss: 335.1248\n",
      "Epoch 2/5\n",
      "636/636 [==============================] - 38s 60ms/step - loss: 181.3067 - val_loss: 76.3865\n",
      "Epoch 3/5\n",
      "636/636 [==============================] - 39s 62ms/step - loss: 34.7309 - val_loss: 10.6795\n",
      "Epoch 4/5\n",
      "636/636 [==============================] - 40s 62ms/step - loss: 4.5290 - val_loss: 1.7445\n",
      "Epoch 5/5\n",
      "636/636 [==============================] - 38s 60ms/step - loss: 1.4112 - val_loss: 1.2609\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense,SimpleRNN\n",
    "\n",
    "modelEE = Sequential()\n",
    "modelEE.add(Embedding(max_words, 32))\n",
    "modelEE.add(SimpleRNN(32))\n",
    "modelEE.add(Dense(2))\n",
    "modelEE.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = modelEE.fit(train_loader.get_preprocessed_features(), train_loader.get_labels(),\n",
    "  batch_size=32,\n",
    "  epochs=5,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.604973  9.42761 ]\n",
      "1.2109075\n"
     ]
    }
   ],
   "source": [
    "evaluator = DataSetEvaluator()\n",
    "print(evaluator.calculateMSE(modelEE, validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.89754052  9.49469686]\n",
      "1.1586700466671107\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
