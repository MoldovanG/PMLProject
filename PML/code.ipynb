{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "class FeatureProcessor:\n",
    "    def __init__(self, tweets):\n",
    "        self.tweets = tweets\n",
    "        self.tokenizer = Tokenizer(num_words=max_words)\n",
    "        # feed our tweets to the Tokenizer\n",
    "        self.tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "        # Tokenizers come with a convenient list of words and IDs\n",
    "        self.dictionary = self.tokenizer.word_index\n",
    "    \n",
    "    def convert_text_to_index_array(self,tweet):\n",
    "        # one really important thing that `text_to_word_sequence` does\n",
    "        # is make all texts the same length -- in this case, the length\n",
    "        # of the longest text in the set.\n",
    "        return [self.dictionary[word] for word in kpt.text_to_word_sequence(tweet)]\n",
    "    def get_tokenized_tweets(self):\n",
    "        allWordIndices = []\n",
    "        # for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "        for tweet in self.tweets:\n",
    "            wordIndices = self.convert_text_to_index_array(tweet)\n",
    "            allWordIndices.append(wordIndices)\n",
    "\n",
    "        # now we have a list of all tweets converted to index arrays.\n",
    "        # cast as an array for future usage.\n",
    "        allWordIndices = np.asarray(allWordIndices)\n",
    "        # create one-hot matrices out of the indexed tweets\n",
    "        train_x = self.tokenizer.sequences_to_matrix(allWordIndices, mode='tfidf')\n",
    "\n",
    "        return train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "import preprocessor as p\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, csv_file_path, mode = 'train'):\n",
    "        data_frame = pd.read_csv(csv_file_path)\n",
    "        self.data = np.array(data_frame)\n",
    "        if mode == 'train' :\n",
    "            self.ids = self.data[:,0]\n",
    "            self.labels = np.asarray(self.data[:,1:3]).astype('float32')\n",
    "            self.features = self.clean_tweets(self.data[:,3])\n",
    "        elif mode == 'test':\n",
    "            self.ids = self.data[:,0]\n",
    "            self.features = self.clean_tweets(self.data[:,1])\n",
    "        print(self.features[0])\n",
    "        feature_processor = FeatureProcessor(self.features)\n",
    "        self.preprocessed_features = feature_processor.get_tokenized_tweets() \n",
    "        self.preprocessed_features = np.asarray(self.preprocessed_features).astype('float32')\n",
    "        \n",
    "    def clean_tweets(self,tweets):\n",
    "        stemmer = SnowballStemmer(\"german\")\n",
    "        german_stop_words = stopwords.words('german')\n",
    "        new_tweets =[]\n",
    "        for tweet in tweets:\n",
    "            tweet = tweet.lower()\n",
    "            tweet = re.sub('[!?,.:\";]', '', tweet)\n",
    "            tweet = p.tokenize(tweet)\n",
    "            cleaned_tweet =[];\n",
    "            separator = \" \"\n",
    "            for word in kpt.text_to_word_sequence(tweet):\n",
    "                if word not in german_stop_words:\n",
    "                    cleaned_tweet.append(stemmer.stem(word))\n",
    "            new_tweets.append(separator.join(cleaned_tweet))\n",
    "        \n",
    "        return np.array(new_tweets)\n",
    "    \n",
    "    def get_data_set(self):\n",
    "        return self.data\n",
    "    def get_ids(self):\n",
    "        return self.ids\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "    def get_preprocessed_features(self):\n",
    "        return self.preprocessed_features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seit d vas kaputt gang bringt numb jahr unglck antwortet de spiegel isch gar nt kaputt gang bringt numb jahr pech dadruf fangt s kondom a lach s'atomkraftwerk au emoji uuuh nd schlecht emoji seisch aner frau isch fett hesch dis leb lang unglck au i fall nm lang lebsch\n",
      "mer aner party bi kolleg en neu bro findt ht sogar dr vo dne emoji\n",
      "emoji min vibi funktionkert nd emoji hesch d'batteri dri gsteckt emoji ja isch nd sgliich\n",
      "(22583,)\n",
      "(22583, 2)\n",
      "(22583,)\n",
      "(3044,)\n",
      "(3044, 2)\n",
      "(3044,)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DatasetLoader('training.csv')\n",
    "validation_loader = DatasetLoader('validation.csv')\n",
    "test_loadet = DatasetLoader('test.csv', mode='test')\n",
    "print(train_loader.get_features().shape)\n",
    "print(train_loader.get_labels().shape)\n",
    "print(train_loader.get_ids().shape)\n",
    "print(validation_loader.get_features().shape)\n",
    "print(validation_loader.get_labels().shape)\n",
    "print(validation_loader.get_ids().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import csv \n",
    "class DataSetEvaluator:\n",
    "    def calculateMSE(self, model, dataSetLoader):\n",
    "        y_pred = model.predict(dataSetLoader.get_preprocessed_features())\n",
    "        print(y_pred[0])\n",
    "        return mean_squared_error(dataSetLoader.get_labels(), y_pred)\n",
    "    def generateEvaluationFile(self, model, dataSetLoader, filename):\n",
    "        y_pred = model.predict(dataSetLoader.get_preprocessed_features())\n",
    "        print(y_pred[0])\n",
    "        ids = dataSetLoader.get_ids()\n",
    "        file=open(filename,\"w\")\n",
    "        writes=csv.writer(file,delimiter=',',quoting=csv.QUOTE_ALL)\n",
    "        count =0\n",
    "        for row in y_pred:\n",
    "            writes.writerow([ids[int(count)],row[0],row[1]])\n",
    "            count = count + 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\george.moldovan\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass kernel=rbf as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-dcd4d6dc6961>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"rbf\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mKRmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKernelRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mKRmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_preprocessed_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\kernel_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\kernel_ridge.py\u001b[0m in \u001b[0;36m_get_kernel\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    132\u001b[0m                       \u001b[1;34m\"degree\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                       \"coef0\": self.coef0}\n\u001b[1;32m--> 134\u001b[1;33m         return pairwise_kernels(X, Y, metric=self.kernel,\n\u001b[0m\u001b[0;32m    135\u001b[0m                                 filter_params=True, **params)\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_kernels\u001b[1;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1938\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown kernel %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1940\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1360\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mrbf_kernel\u001b[1;34m(X, Y, gamma)\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m     \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m     \u001b[0mK\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# exponentiate K in-place\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# To minimize precision issues with float32, we compute the distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;31m# matrix on chunks of X and Y upcast to float64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_euclidean_distances_upcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_euclidean_distances_upcast\u001b[1;34m(X, XX, Y, YY, batch_size)\u001b[0m\n\u001b[0;32m    501\u001b[0m                 \u001b[0md\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mYY_chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mdistances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_slice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_slice\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "alpha = 10 ** -4\n",
    "kernel = \"rbf\"\n",
    "KRmodel = KernelRidge(alpha, kernel)\n",
    "KRmodel.fit(train_loader.get_preprocessed_features(),train_loader.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "LinearRegressionModel = LinearRegression()\n",
    "LinearRegressionModel.fit(train_loader.get_preprocessed_features(),train_loader.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressorChain(base_estimator=BayesianRidge())"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "BayesModel = BayesianRidge()\n",
    "BayesModelWrapper = RegressorChain(BayesModel)\n",
    "BayesModelWrapper.fit(train_loader.get_preprocessed_features(),train_loader.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.70796   9.513377]\n"
     ]
    }
   ],
   "source": [
    "evaluator = DataSetEvaluator()\n",
    "evaluator.generateEvaluationFile(LinearRegressionModel, test_loadet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\george.moldovan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\george.moldovan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RegressorChain(base_estimator=LinearSVR())"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "LinearSvrModel = LinearSVR()\n",
    "# define the chained multioutput wrapper model\n",
    "LinearSvrWrapper = RegressorChain(LinearSvrModel)\n",
    "# fit the model on the whole dataset\n",
    "LinearSvrWrapper.fit(train_loader.get_preprocessed_features(),train_loader.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               256512    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 388,354\n",
      "Trainable params: 388,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "modelNN = Sequential()\n",
    "modelNN.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "modelNN.add(Dropout(0.5))\n",
    "modelNN.add(Dense(256, activation='sigmoid'))\n",
    "modelNN.add(Dropout(0.5))\n",
    "modelNN.add(Dense(2))\n",
    "modelNN.summary()\n",
    "modelNN.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 167.0745 - val_loss: 1.4545\n",
      "Epoch 2/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 10.3987 - val_loss: 1.2774\n",
      "Epoch 3/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 10.1719 - val_loss: 1.2231\n",
      "Epoch 4/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 10.0093 - val_loss: 1.1504\n",
      "Epoch 5/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 9.9659 - val_loss: 1.0727\n",
      "Epoch 6/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 9.6122 - val_loss: 0.9760\n",
      "Epoch 7/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 9.2395 - val_loss: 0.9051\n",
      "Epoch 8/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 9.1802 - val_loss: 0.9039\n",
      "Epoch 9/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 8.9615 - val_loss: 0.9142\n",
      "Epoch 10/10\n",
      "636/636 [==============================] - 1s 2ms/step - loss: 8.8399 - val_loss: 0.9038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f059ec38b0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelNN.fit(train_loader.get_preprocessed_features(), train_loader.get_labels(),\n",
    "  batch_size=32,\n",
    "  epochs=10,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.70891315  9.51027828]\n"
     ]
    }
   ],
   "source": [
    "evaluator = DataSetEvaluator()\n",
    "evaluator.generateEvaluationFile(BayesModelWrapper, test_loadet, 'bayes-submission.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.95694   9.476076]\n",
      "1.1667749\n",
      "[51.99033971  8.57977815]\n",
      "2.0390231118417335\n",
      "[51.537212  9.375402]\n",
      "1.2132088\n",
      "[51.93306739  9.48358665]\n",
      "1.1497289089985046\n"
     ]
    }
   ],
   "source": [
    "evaluator = DataSetEvaluator()\n",
    "print(evaluator.calculateMSE(LinearRegressionModel, validation_loader))\n",
    "print(evaluator.calculateMSE(LinearSvrWrapper, validation_loader))\n",
    "print(evaluator.calculateMSE(modelNN, validation_loader))\n",
    "print(evaluator.calculateMSE(BayesModelWrapper, validation_loader))\n",
    "# print(evaluator.calculateMSE(KRmodel, validation_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "636/636 [==============================] - 39s 61ms/step - loss: 674.9890 - val_loss: 335.1248\n",
      "Epoch 2/5\n",
      "636/636 [==============================] - 38s 60ms/step - loss: 181.3067 - val_loss: 76.3865\n",
      "Epoch 3/5\n",
      "636/636 [==============================] - 39s 62ms/step - loss: 34.7309 - val_loss: 10.6795\n",
      "Epoch 4/5\n",
      "636/636 [==============================] - 40s 62ms/step - loss: 4.5290 - val_loss: 1.7445\n",
      "Epoch 5/5\n",
      "636/636 [==============================] - 38s 60ms/step - loss: 1.4112 - val_loss: 1.2609\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense,SimpleRNN\n",
    "\n",
    "modelEE = Sequential()\n",
    "modelEE.add(Embedding(max_words, 32))\n",
    "modelEE.add(SimpleRNN(32))\n",
    "modelEE.add(Dense(2))\n",
    "modelEE.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = modelEE.fit(train_loader.get_preprocessed_features(), train_loader.get_labels(),\n",
    "  batch_size=32,\n",
    "  epochs=5,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.604973  9.42761 ]\n",
      "1.2109075\n"
     ]
    }
   ],
   "source": [
    "evaluator = DataSetEvaluator()\n",
    "print(evaluator.calculateMSE(modelEE, validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.89754052  9.49469686]\n",
      "1.1586700466671107\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
